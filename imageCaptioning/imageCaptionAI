import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import vocab
import numpy as np

max_caption_length=200

def get_word_from_output(output):
    # Assuming 'vocab' is a dictionary that maps integer indices to words
    # And 'output' is a tensor of shape (batch_size, vocab_size)
    output = output.squeeze(0)  # Remove the batch dimension
    _, max_index = torch.max(output, dim=1)  # Find the index with the highest value
    predicted_word = vocab[max_index.item()]  # Convert index to word
    return predicted_word


# Load a pre-trained ResNet model
resnet = models.resnet50(pretrained=True)
modules = list(resnet.children())[:-1]
resnet = nn.Sequential(*modules)
resnet.eval()

# Load a pre-trained RNN model
# Replace this with a better RNN or transformer-based model
rnn = nn.RNN(input_size=2048, hidden_size=256, num_layers=1, batch_first=True)

# Define the image preprocessing pipeline
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load an image and preprocess it
image = Image.open('example.jpg')
image = transform(image)
image = image.unsqueeze(0)  # Add batch dimension

# Extract image features using the ResNet model
with torch.no_grad():
    image_features = resnet(image)

# Pass image features through the RNN to generate captions
captions = []

# Initialize the hidden state
hidden = torch.zeros(1, 1, 256)

# Generate captions one word at a time
for _ in range(max_caption_length):
    output, hidden = rnn(image_features, hidden)
    predicted_word = get_word_from_output(output)  # Replace with appropriate function
    captions.append(predicted_word)

# Combine the predicted words to form a sentence
caption_sentence = ' '.join(captions)

print(caption_sentence)
